### CH - 09 卷积网络

#### 章节内容

- 卷积运算的基本原理；
- 神经网络中引入卷积操作的动机；
- 池化（Pooling）操作及其作用；
- 神经网络实践中常见的卷积函数变体，以及不同维度数据（如一维、二维、三维）下卷积的应用方式；
- 提升卷积运算效率的常用方法；

> [!tip] 定义
> 由 LeCun 于 1989 年提出，是处理类似网格结构数据（如时间序列的一维网格、图像的二维像素网格 ）的神经网络，因使用卷积这一数学运算得名，且至少一层用卷积运算替代矩阵乘法。

#### 9-1 卷积运算

卷积就像一种“智能移动平均”，它通过一个滑动的权重模板，持续地对历史数据进行加权求和，从而得到一个更平滑、更可靠的当前状态估计。

> [!note] 场景引入  
> 用激光传感器追踪宇宙飞船位置，$x(t)$ 表示飞船时刻 $t$ 的位置（实值，可任意时刻读取 ），但传感器受噪声干扰，需对测量结果加权平均以得到低噪声位置估计。   
> **数学表达**：
> 采用加权函数 $w(a)$（$a$ 为测量结果距当前时刻的时间间隔，近的测量结果权重高 ），对任意时刻做加权平均，得到平滑估计函数 $s(t)$：  
> $$s(t) = \int x(a)w(t - a)da$$  
> 
> 此运算即卷积，用星号表示：  
> $$s(t) = (x * w)(t)$$  
> **约束条件**：
>
> - $w$ 需是有效概率密度函数，保证输出为加权平均。  
> - $w$ 在参数为负时取值为 0，避免预测“未来”（不符合实际测量逻辑 ），但这是示例限制，卷积定义更通用。  

> [!note] 术语对应  
> 
> - **输入**：卷积的第一个参数（如示例中 $ x $，代表原始数据 ）。  
> - **核函数**：卷积的第二个参数（如示例中 $ w $，用于对输入加权 ）。  
> - **特征映射**：卷积运算的输出（如示例中 $ s(t) $，是提取后的特征表示 ）。  

我们经常一次在多个维度上进行卷积运算。

例如把一张二维的图像 $ I $ 作为输入，我们就需要使用一个二维的核 $ K $：  

$$
S(i,j) = (I * K)(i,j) = \sum_{m} \sum_{n} I(m,n) K(i - m, j - n)
$$  

卷积是可交换的 (commutative)，我们可以等价地写作：  

$$
S(i,j) = (K * I)(i,j) = \sum_{m} \sum_{n} I(i - m, j - n) K(m,n)
$$  

许多深度学习框架（如 PyTorch、TensorFlow）实际实现的是**互相关函数**，而非严格数学定义的卷积：  

$$
S(i,j) = (I \star K)(i,j) = \sum_{m} \sum_{n} I(i + m, j + n) K(m,n)
$$  

> [!tip] 与卷积的区别  
> 
> - **核心差异**：互相关不翻转核，直接用核 $ K(m,n) $ 与图像 $ I(i + m, j + n) $ 对齐计算。  
> - **工程意义**：实现更简单（无需处理核的翻转），且能等价完成“用核提取图像特征”的任务。  
> - **等价性**：若将核 $ K $ 预先翻转，互相关可等价于卷积。因此，工程中常用互相关替代卷积，简化计算。  

![alt text](image-3.png)

##### 离散卷积

###### 矩阵乘法表示

设：

- 输入序列 $ x = [x_0, x_1, \dots, x_{N - 1}] $，
- 核序列 $ w = [w_0, w_1, \dots, w_{M - 1}] $，
- 卷积结果 $ s $ 的长度为 $ L = N + M - 1 $，
- 其离散卷积计算为 $ s(t) = \sum_{a = -\infty}^{\infty} x(a)w(t - a) $，实际有效计算是有限项求和。

为用矩阵乘法表示，需构建**卷积矩阵 $ C $**，使得 $ s = Cx $（这里做了向量展开等处理，把输入、输出转成列向量 ）。比如 $ N = 3 $（$ x_0, x_1, x_2 $ ）、$ M = 2 $（$ w_0, w_1 $ ），卷积结果 $ s $ 长度为 $ 3 + 2 - 1 = 4 $，卷积矩阵 $ C $ 构造如下：

\[
C = \begin{bmatrix}
w_0 & 0 & 0 \\
w_1 & w_0 & 0 \\
0 & w_1 & w_0 \\
0 & 0 & w_1
\end{bmatrix}
\]

此时 $ x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix} $，$ Cx $ 计算后得到的列向量就是卷积结果 $ s $，对应每个 $ s(t) $ 是矩阵行与 $ x $ 的点积。

这种矩阵叫做 **Toeplitz 矩阵（Toeplitz matrix）**

#### 9-2 动机

卷积运算通过三个重要的思想来帮助改进机器学习系统：**稀疏交互（sparse interactions）、参数共享（parameter sharing）、等变表示（equivariant representations）**。

##### 稀疏交互（sparse interactions）

传统神经网络用**矩阵乘法**建立输入 - 输出连接，特点是：

- 参数与交互的关系：参数矩阵里每个参数，对应 “一个输入单元 ↔ 一个输出单元” 的交互。
- 结果：每个输出单元都要和所有输入单元交互（全连接）。


处理 1000×1000 像素的图像（输入 $ m = 10^6 $ ），若输出单元 $ n = 100 $，传统全连接需要 $ 10^6 \times 100 = 10^8 $ 个参数，计算量是 $ O(10^8) $ —— 参数多到爆炸，计算慢、存储难！

卷积网络通过让**核（卷积核）** 的大小远小于输入大小，实现稀疏交互，特点是：  

- **局部连接逻辑**：输出单元（特征）仅与输入的局部区域交互，而非全部输入。  

