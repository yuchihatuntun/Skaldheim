### Reference

### Progress

| 章节 | 教材 | 代码复现 |
| ---- | ---- | ---- |
| 2.1 数据操作 | ✅ |  |
| 2.2 数据预处理 | ✅ |  |
| 2.3 线性代数 | ✅ |  |
| 2.4 微积分 | ✅ |  |
| 2.5 自动微分 | ✅ |  |
| 2.6 概率 | ✅ |  |
| 2.7 查阅文档 | ✅ |  |
| 3.1 线性回归 | ✅ |  |
| 3.2 线性回归的从零开始实现 | ✅ |  |
| 3.3 线性回归的简洁实现 | ✅ |  |
| 3.4 softmax回归 |✅  |  |
| 3.5 图像分类数据集 |  |  |
| 3.6 softmax回归的从零开始实现 |  |  |
| 3.7 softmax回归的简洁实现 |  |  |
| 4.1 多层感知机 |  |  |
| 4.2 多层感知机的从零开始实现 |  |  |
| 4.3 多层感知机的简洁实现 |  |  |
| 4.4 模型选择、欠拟合和过拟合 |  |  |
| 4.5 权重衰减 |  |  |
| 4.6 暂退法(Dropout) |  |  |
| 4.7 前向传播、反向传播和计算图 |  |  |
| 4.8 数值稳定性和模型初始化 |  |  |
| 4.9 环境和分布偏移 |  |  |
| 4.10 实战Kaggle比赛:预测房价 |  |  |
| 5.1 和块 |  |  |
| 5.2 参数管理 |  |  |
| 5.3 延后初始化 |  |  |
| 5.4 自定义层 |  |  |
| 5.5 读写文件 |  |  |
| 5.6 GPU |  |  |
| 6.1 从全连接层到卷积 |  |  |
| 6.2 图像卷积 |  |  |
| 6.3 填充和步幅 |  |  |
| 6.4 多输入多输出通道 |  |  |
| 6.5 汇聚层 |  |  |
| 6.6 卷积神经网络(LeNet) |  |  |
| 7.1 深度卷积神经网络(AlexNet) |  |  |
| 7.2 使用块的网络(VGG) |  |  |
| 7.3 网络中的网络(NiN) |  |  |
| 7.4 含并行连结的网络(GoogLeNet) |  |  |
| 7.5 批量规范化 |  |  |
| 7.6 残差网络(ResNet) |  |  |
| 7.7 稠密连接网络(DenseNet) |  |  |
| 8.1 序列模型 |  |  |
| 8.2 文本预处理 |  |  |
| 8.3 语言模型和数据集 |  |  |
| 8.4 循环神经网络 |  |  |
| 8.5 循环神经网络的从零开始实现 |  |  |
| 8.6 循环神经网络的简洁实现 |  |  |
| 8.7 通过时间反向传播 |  |  |
| 9.1 门控循环单元(GRU) |  |  |
| 9.2 长短期记忆网络(LSTM) |  |  |
| 9.3 深度循环神经网络 |  |  |
| 9.4 双向循环神经网络 |  |  |
| 9.5 机器翻译与数据集 |  |  |
| 9.6 编码器-解码器架构 |  |  |
| 9.7 序列到序列学习(seq2seq) |  |  |
| 9.8 束搜索 |  |  |
| 10.1 注意力提示 |  |  |
| 10.2 注意力汇聚:Nadaraya-Watson核回归 |  |  |
| 10.3 注意力评分函数 |  |  |
| 10.4 Bahdanau注意力 |  |  |
| 10.5 多头注意力 |  |  |
| 10.6 自注意力和位置编码 |  |  |
| 10.7 Transformer |  |  |
| 11.1 优化和深度学习 |  |  |
| 11.2 梯度下降 |  |  |
| 11.3 随机梯度下降 |  |  |
| 11.4 小批量随机梯度下降 |  |  |
| 11.5 动量法 |  |  |
| 11.6 AdaGrad算法 |  |  |
| 11.7 RMSProp算法 |  |  |
| 11.8 Adadelta |  |  |
| 11.9 Adam算法 |  |  |
| 11.10 学习率调度器 |  |  |
| 12.1 编译器和解释器 |  |  |
| 12.2 异步计算 |  |  |
| 12.3 自动并行 |  |  |
| 12.4 硬件 |  |  |
| 12.5 多GPU训练 |  |  |
| 12.6 多GPU的简洁实现 |  |  |
| 12.7 参数服务器 |  |  |
| 13.1 图像增广 |  |  |
| 13.2 微调 |  |  |
| 13.3 目标检测和边界框 |  |  |
| 13.4 锚框 |  |  |
| 13.5 多尺度目标检测 |  |  |
| 13.6 目标检测数据集 |  |  |
| 13.7 单发多框检测(SSD) |  |  |
| 13.8 区域卷积神经网络(R-CNN)系列 |  |  |
| 13.9 语义分割和数据集 |  |  |
| 13.10 转置卷积 |  |  |
| 13.11 全卷积网络 |  |  |
| 13.12 风格迁移 |  |  |
| 13.13 实战Kaggle比赛:图像分类(CIFAR-10) |  |  |
| 13.14 实战Kaggle比赛:狗的品种识别(ImageNet Dogs) |  |  |
| 14.1 词嵌入(word2vec) |  |  |
| 14.2 近似训练 |  |  |
| 14.3 用于预训练词嵌入的数据集 |  |  |
| 14.4 预训练word2vec |  |  |
| 14.5 全局向量的词嵌入(Glove) |  |  |
| 14.6 子词嵌入 |  |  |
| 14.7 词的相似性和类比任务 |  |  |
| 14.8 来自Transformers的双向编码器表示(BERT) |  |  |
| 14.9 用于预训练BERT的数据集 |  |  |
| 14.10 预训练BERT |  |  |
| 15.1 情感分析及数据集 |  |  |
| 15.2 情感分析:使用循环神经网络 |  |  |
| 15.3 情感分析:使用卷积神经网络 |  |  |
| 15.4 自然语言推断与数据集 |  |  |
| 15.5 自然语言推断:使用注意力 |  |  |
| 15.6 针对序列级和词元级应用微调BERT |  |  |
| 15.7 自然语言推断:微调BERT |  |  |

### CH - 06 卷积神经网络

传统方法将图像展平为一维向量输入全连接网络，忽略了图像的**空间结构信息**（如相邻像素关联性），处理效率不足。因此需要利用像素间关联的更优模型。

**CNN的优势与拓展**：

- **技术优势**：参数少于全连接网络，易通过GPU并行计算，兼具**高效采样（模型精确）和高效计算**能力。  
- **设计渊源**：受益于生物学、群论及实验启发。  
- **应用拓展**：除图像外，还渗透到音频、文本、时间序列（传统循环网络场景），甚至经调整后应用于**图结构数据、推荐系统**。  

> [!tip] 内容规划  
> 
> - **基础元素**：讲解卷积层、填充（padding）、步幅（stride）、汇聚层（pooling）、多通道（channel），及现代CNN架构设计。  
> 
> - **经典模型**：介绍**LeNet**（首个成功应用的CNN，早于现代深度学习兴起）。  
> 
> - **后续衔接**：下一章深入现代流行CNN架构（覆盖经典技术）。  

简言之，本章围绕CNN的**原理、优势、基础组件**及经典模型展开，为理解现代计算机视觉技术奠基。

### CH - 07 现代卷积神经网络

本章聚焦 **现代CNN架构**（许多研究以此为基础）。  

**ImageNet竞赛** 是关键参照：自2010年起，作为计算机视觉监督学习进展的“风向标”，本章模型多为该竞赛优胜者。  

**核心模型及特点**：

| 模型名称       | 核心设计特点                                                                 |  
|----------------|------------------------------------------------------------------------------|  
| **AlexNet**    | 首个在大规模视觉竞赛中击败传统模型的 **大型CNN**，开启深度学习在CV的统治时代。 |  
| **VGG**        | 采用 **重复神经网络块** 构建，结构简洁且规律。                               |  
| **NiN**        | 用 **卷积层+1×1卷积层** 替代全连接层，深化网络表达。                         |  
| **GoogLeNet**  | 引入 **并行连结**，通过不同窗口的卷积层+最大汇聚层，并行抽取多尺度信息。     |  
| **ResNet**     | 借助 **残差块** 搭建跨层数据通道，解决深层网络退化问题，是CV领域最流行架构。 |  
| **DenseNet**   | 计算成本高，但通过 **稠密连接** 实现更强特征复用，效果优异。                 |  

**底层逻辑**： 

- 深度CNN概念简单（“神经网络堆叠”），但 **架构设计、超参数选择** 会极大影响性能。  
- 这些模型是 **人类直觉（领域经验）+数学洞察+大量试错** 后的成果。  


