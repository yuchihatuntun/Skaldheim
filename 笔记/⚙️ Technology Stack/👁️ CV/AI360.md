### 绪论

### 前置知识

### 滤波器与卷积核

#### 图像直方图

##### 图像的本质：像素矩阵

一张数字图像，尤其是我们这里讨论的**灰度图**，可以被理解为一个二维矩阵。 矩阵中的每一个元素都代表一个像素，其数值被称为像素值或灰度值，表示该点的亮度。 通常，这个值用一个字节（8位）来表示，范围从0到255，其中0代表最暗的黑色，255代表最亮的白色。

##### 什么是图像直方图

图像直方图是一种图形化的表示方法，它显示了一张图像中亮度（灰度）值的分布频率，即统计从0到255的每一个灰度级在这张图像中总共出现了多少次。

<span style="background:rgba(252, 163, 180, 0.55)">将二维的像素空间变换为一维的统计分布</span>。  这让我们能从一个宏观的、统计学的角度来理解图像的整体色调分布。

##### 应用实例

###### 辅助目标检测，减少误报

在目标检测任务中，算法可能会将一些背景区域误判为目标（例如，将一块纹理复杂的背景识别为人脸）。 此时，直方图可以提供一个有效的<span style="background:rgba(252, 163, 180, 0.55)">判别依据</span>。

###### 辅助医疗诊断

#### 图像变换函数

我们将研究如何主动地**修改**这些像素值，从而创造出一张新的图像。

> [!note] 函数定义
> 一个图像可以被定义为一个函数 $f$。这个函数的作用是将一个二维的像素坐标 $(x, y)$ 映射到一个灰度值。

> [!note] 数学表达
> 对于一个矩形图像，这个函数关系可以写成  
> $$
> f:[a,b]\times[c,d]\to[0,255]
> $$
> 其中 $[a,b]\times[c,d]$ 表示图像的坐标范围（宽度和高度），$[0,255]$ 是函数输出的范围，代表可能的灰度值。

**彩色图像**：这个概念同样适用，只是函数的输出不再是一个单一的数值，而是一个包含红 (R)、绿 (G)、蓝 (B) 三个通道值的向量：
$$
f(x, y) =
\begin{bmatrix}
r(x, y) \\
g(x, y) \\
b(x, y)
\end{bmatrix}
$$

**直方图本身就是图像变换函数的一种**。它将整个二维坐标空间 $(x,y)$ 的信息变换（或说聚合）成了一个一维的统计分布。

##### 变换操作：对函数进行运算

###### 对像素值进行运算

例如：

$$g(x,y)=f(x,y)+20$$  

这个操作会遍历每一个坐标 $(x,y)$，读取原始像素值 $f(x,y)$，然后将其增加20，得到新图像的像素值。这种操作会使整张图片变亮。这类只依赖当前点像素值的变换，我们称之为**点操作**。更多点操作的例子，如通过 $255-x$ 实现颜色翻转，或通过 $x\cdot2$ 来增强对比度。

###### 对坐标进行运算

例如：  
$$g(x,y)=f(-x,y)$$  
这个操作在计算新图像 $(x,y)$ 位置的像素值时，会去采样原始图像 $(-x,y)$ 位置的像素值。其最终效果是将图像**水平翻转**。

如果我们想让新像素值不仅与当前点有关，**还受到其周围邻近像素的影响**，应该怎么做呢？

#### 图像滤波

##### 什么是图像滤波，为何需要它

> [!note] 图像滤波定义
> 图像滤波是生成新图像的过程，其中每个新像素的值由原始图像对应位置及其局部邻域的像素值共同决定。也就是说，计算新图像某一点时，需要参考该点及其周围的一小块区域的像素信息。

> [!note] 目的
> 我们进行滤波是为了从图像中提取有用信息或增强图像，包括：
> - **增强与美化**：去除噪声、平滑或锐化图像，使其更清晰。
> - **特征提取**：提取物体的边缘或轮廓，为后续形状识别提供基础。

##### 核心机制：卷积核 (Kernel) 与卷积 (Convolution)


### 边缘检测

### Course 10 注意力和Transformers

> [!tip] 前置问题
> 1. 从NLP跨界到CV的里程碑工作是什么？
> 2. BERT、GPT-3、T5这些模型都是什么机制？
> 3. 这两种注意力机制有什么共同点和差异？

#### 注意力(空间特征 v.s. 注意力)

### 生成模型

> [!tip] 前置问题
> 1. 自编码器（AE）是什么？AE和变分自编码器（VAE）有什么区别？
> 2. GAN的灵感来源是什么？
> 3. 生成式模型与判别式模型的区别是什么？

#### PixelCNN

##### 完全可见置信网络 (Fully Visible Belief Networks, FVBN)

> [!note] FVBN的概率分解机制
> FVBN 将 $n$ 维观测变量 $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ 的联合概率 $p(\mathbf{x})$ 分解为一系列条件概率的乘积：
>
> $$
> p(\mathbf{x}) = \prod_{i=1}^{n} p(x_i \mid x_1, x_2, \ldots, x_{i-1})
> $$
>
> 其中，每个条件概率 $p(x_i \mid x_{<i})$ 表示当前变量 $x_i$ 对之前所有变量 $x_1$ 到 $x_{i-1}$ 的依赖关系。这种分解方式基于概率图模型中的完全有向图结构，所有节点（变量）之间均存在显式的依赖连接。
>
> 即**按照固定的顺序一块一块地拼图，而且每一步都要计算下一块拼图该怎么放**。

其属于**显式密度建模 (Explicit Density Modeling)** 的范畴，

> [!note] 定义
> 显式密度建模是指通过数学形式显式地定义概率密度函数 $p_{\text{model}}(x; \theta)$，其中 $\theta$ 为模型参数。该密度函数能够直接计算观测数据 $x$ 的似然值，并通过最大化似然估计（MLE）或贝叶斯推断等方法优化参数，使得模型分布 $p_{\text{model}}$ 尽可能接近真实数据分布 $p_{\text{data}}$。

$$p(x) = p(x_1, x_2, \ldots, x_n)$$

其中，图像`x`被视为一个`n`维的随机向量，`x₁`到`xₙ`是它的各个分量，即图像中的每一个像素。

因此，$p(x)$就是所有`n`个像素的**联合概率分布**。该模型的挑战在于，图像像素之间存在着复杂的空间依赖关系，导致这个联合分布的<span style="background:rgba(252, 163, 180, 0.55)">维度极高</span>且<span style="background:rgba(252, 163, 180, 0.55)">结构复杂</span>。

但是一口气计算这个联合分布概率实在是困难，我们不一口气画完整张图，而是**一个像素一个像素地画**。就像写文章时一个词一个词地写，即通过 **链式法则 (Chain Rule)** 实现对显式概率密度的建模

分解后的形式为：

$$
p(x) = \prod_{i=1}^{n} p(x_i \mid x_1, \ldots, x_{i-1})
$$

这个公式将一个难以处理的高维联合概率，精确地转换为了`n`个一维条件概率的乘积 。

![alt text](image-2.png)

###### 自回归建模 (Autoregressive Modeling)

> [!note] 定义
> 给定一个 $D$ 维随机变量 $\mathbf{x} = (x_1, x_2, \ldots, x_D)$，其联合概率分布 $p(\mathbf{x})$ 被分解为：
> $$
> p(\mathbf{x}) = \prod_{i=1}^{D} p(x_i \mid x_1, x_2, \ldots, x_{i-1})
> $$
> 其中 $p(x_i \mid x_{<i})$ 表示变量 $x_i$ 在给定所有前序变量 $x_1, \ldots, x_{i-1}$ 下的条件概率。  
> 这种分解方式称为**自回归分解（Autoregressive Decomposition）**，因为每个变量“回归”于其前面的变量。

- 该分解具有**自回归**的性质：第 $i$ 个像素 $x_i$ 的概率分布，只依赖于它之前的所有像素 $x_1$ 至 $x_{i-1}$ 。这为生成过程定义了一个明确的、有序的依赖关系和计算顺序，如图所示，从左上角第一个像素 $x_1$ 开始，依次生成直到最后一个像素 $x_n$ 。

- 每一个条件概率 $p(x_i \mid x_1, \ldots, x_{i-1})$ 都是一个一维的概率分布，其**参数由之前的像素值（即条件）所决定**。

###### 参数化与优化 (Parameterization and Optimization):

- **参数化**：像素值之间的条件依赖关系极其复杂，无法用简单的函数来描述。因此，我们使用神经网络来对这些条件概率分布进行参数化。神经网络的强大拟合能力使其可以学习从条件（$x_1$ 到 $x_{i-1}$）到目标像素 $x_i$ 分布的复杂映射。

- **优化**：模型的训练目标是**最大化训练数据的对数似然 (Maximize Log-Likelihood)**。通过在整个训练集上最大化 $\log p(\mathbf{x})$，模型被驱动去学习能够最好地解释观测数据的参数，从而捕获真实数据的内在分布。

##### PixelCNN

> [!note] 定义
> 给定一张图像 $\mathbf{x}$（尺寸为 $n \times n \times C$，其中 $C$ 为通道数，如 RGB 图像的 $C = 3$），PixelCNN 将联合概率 $p(\mathbf{x})$ 分解为序列条件概率的乘积：
> $$
> p(\mathbf{x}) = \prod_{i=1}^{n^2} p(x_i \mid x_1, x_2, \ldots, x_{i-1})
> $$
> 其中：
> - $x_i$ 表示按光栅扫描顺序（左上到右下）排列的第 $i$ 个像素；
> - 每个 $p(x_i \mid x_{<i})$ 是给定前序像素时当前像素的条件概率，通过神经网络建模。
>
> 对于 RGB 图像，条件概率进一步分解为通道间的依赖关系：
> $$
> p(x_i \mid x_{<i}) = p(x_{i,R} \mid x_{<i}) \cdot p(x_{i,G} \mid x_{<i}, x_{i,R}) \cdot p(x_{i,B} \mid x_{<i}, x_{i,R}, x_{i,G})
> $$
> 即 R 通道仅依赖前序像素，G 通道额外依赖当前像素的 R 值，B 通道依赖 R 和 G 值。

###### 核心机制：带掩码的卷积 (Masked Convolution)

- 与前一页提出的基于链式法则的自回归思想一致，PixelCNN 旨在对每个像素 $x_i$ 的条件概率分布 $p(x_i \mid x_{<i})$ 进行建模。
- 它创新性地使用**卷积神经网络 (CNN)** 来捕获这种条件依赖性。为保证自回归特性——即模型在预测 $x_i$ 时只能利用其上下文 (causal context)，即已经生成的像素——PixelCNN采用了一种特殊的**带掩码的卷积 (Masked Convolution)**。
- 这种掩码将卷积核中对应于当前及<span style="background:rgba(252, 163, 180, 0.55)">未来像素位置的权重置为零</span>，从而确保了信息流的单向性，严格遵守了概率的有序分解。

###### 与PixelRNN的对比与优势

- PixelRNN使用循环神经网络（RNN）来建模像素间的序列依赖关系，其计算是严格串行的。
- PixelCNN利用卷积操作，可以在**训练阶段**并行计算所有像素位置的条件分布。这是因为训练时整张图像是已知的，卷积可以在整个图像上一次性完成。这使得其训练效率远高于PixelRNN。

###### 主要局限性：串行生成过程 (Sequential Generation)

- 尽管训练过程可以并行化，但其**生成（采样）过程**仍然是严格串行的。
- 为了生成一个新样本，模型必须从 $x_1$ 开始，依次生成 $x_2$，$x_3$，… 直至 $x_n$。每生成一个像素，都需要将之前所有生成的像素作为输入，进行一次完整的前向传播。
- 因此，生成一张 $N \times N$ 的图像需要进行 $N^2$ 次网络前向传播。例如，对于一张32x32的图像，这需要1024次前向传播，导致其生成速度极其缓慢，这是该方法在实际应用中的一个主要瓶颈。
